import os
import re
import io
import datetime
from urllib.parse import quote

import arxiv
import openai
import requests
from notion_client import Client
from pytz import timezone
from pydub import AudioSegment


NOTION_TOKEN = os.environ["NOTION_TOKEN"]
DATABASE_ID = os.environ["DATABASE_ID"]
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]

SEMANTIC_SCHOLAR_API_KEY = os.environ.get("SEMANTIC_SCHOLAR_API_KEY", "").strip()

GITHUB_USER = "Choihyunseok1"
GITHUB_REPO = "CV_Papers_Podtcast_bot"

notion = Client(auth=NOTION_TOKEN)
client = openai.OpenAI(api_key=OPENAI_API_KEY)

# Top10ë§Œ ìƒì„±
TOP_K = 10
ARXIV_MAX_RESULTS = 500

# ë³¸ë¬¸ ìƒì„± ë°°ì¹˜ í¬ê¸°
BATCH_SIZE_FULL = 2

# OpenAI ì¶œë ¥ ê¸¸ì´
MAX_OUT_TOKENS_SUMMARY = 4000
MAX_OUT_TOKENS_FULL_PER_BATCH = 2800

# TTS
TTS_MODEL = "tts-1-hd"
TTS_VOICE = "onyx"
TTS_SPEED = 1.25
TTS_CHUNK_CHARS = 2000
TTS_CHUNK_OVERLAP = 0

# arXiv announce ê¸°ì¤€ ì‹œê° (Eastern Time 20:00)
ARXIV_ANNOUNCE_HOUR_ET = 20
ARXIV_ANNOUNCE_MINUTE_ET = 0


S2_API_BASE = "https://api.semanticscholar.org/graph/v1"
S2_FIELDS = ",".join([
    "title",
    "authors.name",
    "authors.hIndex",
    "authors.paperCount",
    "authors.citationCount",
    "externalIds",
    "url",
    "openAccessPdf",
    "citationCount",
    "venue",
    "year",
])


def s2_headers():
    headers = {"Content-Type": "application/json"}
    if SEMANTIC_SCHOLAR_API_KEY:
        headers["x-api-key"] = SEMANTIC_SCHOLAR_API_KEY
    return headers


def chunk_list(xs, n):
    out = []
    for i in range(0, len(xs), n):
        out.append(xs[i:i + n])
    return out


def arxiv_id_from_entry_id(entry_id: str) -> str:
    if not entry_id:
        return ""
    return entry_id.rstrip("/").split("/")[-1].strip()


def fetch_s2_papers_batch(arxiv_ids):
    results = {}
    ids = [f"ARXIV:{aid}" for aid in arxiv_ids if aid]
    if not ids:
        return results

    url = f"{S2_API_BASE}/paper/batch?fields={quote(S2_FIELDS)}"
    body = {"ids": ids}

    try:
        resp = requests.post(url, headers=s2_headers(), json=body, timeout=30)
    except Exception as e:
        print("S2 request error:", str(e))
        return results

    if resp.status_code != 200:
        print("S2 batch fetch failed:", resp.status_code, (resp.text or "")[:300])
        return results

    data = resp.json() or []
    for item in data:
        if not item:
            continue
        ext = (item.get("externalIds") or {})
        arxiv_id = (ext.get("ArXiv") or "").strip()
        if arxiv_id:
            results[arxiv_id] = item
    return results


def get_last_announce_window_et(now_et):
    # now_etì´ 20:00 ET ì´í›„ë©´: window_start = ì˜¤ëŠ˜ 20:00
    # now_etì´ 20:00 ET ì´ì „ì´ë©´: window_start = ì–´ì œ 20:00
    today_announce = now_et.replace(
        hour=ARXIV_ANNOUNCE_HOUR_ET,
        minute=ARXIV_ANNOUNCE_MINUTE_ET,
        second=0,
        microsecond=0
    )
    if now_et >= today_announce:
        window_start = today_announce
    else:
        window_start = today_announce - datetime.timedelta(days=1)

    window_end = window_start + datetime.timedelta(days=1)
    return window_start, window_end


def split_notion_text(text, max_len=1900):
    text = (text or "").strip()
    if not text:
        return []
    return [text[i:i + max_len] for i in range(0, len(text), max_len)]


def chunk_text_by_chars(text, chunk_chars=2000, overlap=0):
    text = (text or "").strip()
    if not text:
        return []
    chunks = []
    i = 0
    n = len(text)
    step = max(1, chunk_chars - overlap)
    while i < n:
        chunk = text[i:i + chunk_chars].strip()
        if chunk:
            chunks.append(chunk)
        i += step
    return chunks


def build_papers_info(papers):
    papers_info = ""
    for i, p in enumerate(papers):
        papers_info += f"ë…¼ë¬¸ {i+1} ì œëª©: {p.title}\nì´ˆë¡: {p.summary}\n\n"
    return papers_info


def prompt_summary_and_3min(valid_papers):
    papers_info = build_papers_info(valid_papers)

    return f"""
ì•„ë˜ëŠ” ì˜¤ëŠ˜ arXivì— ìƒˆë¡œ ê³µê°œëœ {len(valid_papers)}ê°œì˜ ì»´í“¨í„° ë¹„ì „ ë…¼ë¬¸ì…ë‹ˆë‹¤.

{papers_info}

ìœ„ ë…¼ë¬¸ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.

1) [ìš”ì•½]
- ë…¸ì…˜ ê¸°ë¡ìš© í•µì‹¬ ìš”ì•½ì…ë‹ˆë‹¤.
- ê° ë…¼ë¬¸ë³„ë¡œ ì œëª©ì„ ì–¸ê¸‰í•˜ê³ , '-í•¨', '-ì„' í˜•íƒœì˜ ì§§ì€ ìš”ì•½ì²´ë¡œ 3ì¤„ì”© ì‘ì„±í•´ ì£¼ì„¸ìš”.
- í•œ ì¤„ì´ ëë‚˜ë©´ ë°˜ë“œì‹œ ì—”í„°ë¡œ êµ¬ë¶„í•´ ì£¼ì„¸ìš”.
- ê° ë…¼ë¬¸ ìš”ì•½ ì‹œì‘ì€ '1. (ë…¼ë¬¸ì œëª©)' í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ë§Œ ë¶™ì—¬ ì£¼ì„¸ìš”.
- ë…¼ë¬¸ë“¤ ì‚¬ì´ëŠ” ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„í•´ ì£¼ì„¸ìš”.

2) [3ë¶„ëŒ€ë³¸]
- "ì‹œê°„ì´ ì—†ìœ¼ì‹  ë¶„ë“¤ì„ ìœ„í•œ 3ë¶„ í•µì‹¬ ìš”ì•½ì…ë‹ˆë‹¤"ë¡œ ì‹œì‘í•´ ì£¼ì„¸ìš”.
- ëª¨ë“  ë…¼ë¬¸ì„ ë¹ ì§ì—†ì´ í¬í•¨í•´ ì£¼ì„¸ìš”.
- ê° ë…¼ë¬¸ ì œëª©ì„ ë§í•œ ë’¤, ë…¼ë¬¸ ë‹¹ ì•½ 350~450ì ë‚´ì™¸ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
- ì „ì²´ ê¸¸ì´ëŠ” ì•½ 3ë¶„(Â±15ì´ˆ) ë¶„ëŸ‰ì´ ë˜ë„ë¡ ì¡°ì ˆí•´ ì£¼ì„¸ìš”.
- ë…¼ë¬¸ ìˆ˜ê°€ ë§ì„ ê²½ìš° ê° ë…¼ë¬¸ì˜ ì„¤ëª… ê¸¸ì´ë¥¼ ìë™ìœ¼ë¡œ ì¤„ì—¬ ì „ì²´ ë¶„ëŸ‰ì„ ìœ ì§€í•´ ì£¼ì„¸ìš”.
- ë…¼ë¬¸ ì œëª©ì€ ë°˜ë“œì‹œ ì˜ë¬¸ìœ¼ë¡œ í‘œê¸°í•˜ë˜, ì œëª©ì˜ íŠ¹ìˆ˜ ê¸°í˜¸(:, -, +, / ë“±)ëŠ” ì‰¼í‘œ(,)ë¡œ ë°”ê¿” ì£¼ì„¸ìš”.
- CNN, ViT, GAN, SOTA ë“± ì•½ì–´ëŠ” ì˜ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
- diffusion, transformer, attention, encoder, decoder, latent, alignment, distillation, benchmark, dataset ê°™ì€ ì „ë¬¸ ìš©ì–´ëŠ” ë²ˆì—­í•˜ì§€ ë§ê³  ì˜ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
- ì‰¼í‘œ(,)ë¥¼ ì¶©ë¶„íˆ ì‚¬ìš©í•´ í˜¸í¡ ì§€ì ì„ í‘œì‹œí•´ ì£¼ì„¸ìš”.
- ë™ë£Œ ì—°êµ¬ìì—ê²Œ ì„¤ëª…í•˜ë“¯ ì°¨ë¶„í•œ êµ¬ì–´ì²´ë¡œ ì“°ë˜, ë°˜ë“œì‹œ ê³µì ì¸ ë¼ë””ì˜¤ í†¤ì˜ ì¡´ëŒ“ë§ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
- ë°˜ë§, êµ¬ì–´ì²´ ì¶•ì•½, ì¹œê·¼í•œ ëŒ€í™”ì²´(ì˜ˆ: ~í•´ìš”, ~í–ˆì£ )ëŠ” ì‚¬ìš©í•˜ì§€ ë§ì•„ ì£¼ì„¸ìš”.

[ë§ˆë¬´ë¦¬ ê·œì¹™]
- ëª¨ë“  ë…¼ë¬¸ ì„¤ëª…ì´ ëë‚œ ë’¤, ì•„ë˜ í†¤ì˜ ì•„ì›ƒíŠ¸ë¡œ ë©˜íŠ¸ë¥¼ í•œ ë¬¸ë‹¨ìœ¼ë¡œ ì¶”ê°€í•´ ì£¼ì„¸ìš”.
- ê°ì‚¬ ì¸ì‚¬ë‚˜ ì¼ìƒì ì¸ ì¸ì‚¿ë§ì€ ì‚¬ìš©í•˜ì§€ ë§ì•„ ì£¼ì„¸ìš”.
- ë” ìì„¸í•œ ë‚´ìš©ì´ ì „ì²´ ë¸Œë¦¬í•‘ì— ìˆë‹¤ëŠ” ì ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì•ˆë‚´í•´ ì£¼ì„¸ìš”.

ì•„ì›ƒíŠ¸ë¡œ ì˜ˆì‹œ í†¤:
"ë³´ë‹¤ ìì„¸í•œ ë‚´ìš©ì€ ì „ì²´ ë¸Œë¦¬í•‘ì—ì„œ ì´ì–´ì„œ ë‹¤ë£¹ë‹ˆë‹¤.
ì§€ê¸ˆê¹Œì§€ ì˜¤ëŠ˜ì˜ ì»´í“¨í„° ë¹„ì „ ë…¼ë¬¸ 3ë¶„ í•µì‹¬ ìš”ì•½ì´ì—ˆìŠµë‹ˆë‹¤."

ì¶œë ¥ í˜•ì‹:
[ìš”ì•½]
(ë‚´ìš©)

[3ë¶„ëŒ€ë³¸]
(ë‚´ìš©)
""".strip()


def prompt_full_body_for_batch(batch_papers, batch_index, total_batches, start_index):
    papers_info = build_papers_info(batch_papers)

    return f"""
ì•„ë˜ëŠ” ì»´í“¨í„° ë¹„ì „ ë…¼ë¬¸ ë°°ì¹˜ {batch_index}/{total_batches}ì…ë‹ˆë‹¤.
ì´ ë°°ì¹˜ì˜ ë…¼ë¬¸ ì „ì—­ ë²ˆí˜¸ëŠ” {start_index}ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.

{papers_info}

ì¤‘ìš”:
- ì§€ê¸ˆì€ ë°©ì†¡ì˜ ë„ì…ë¶€ì™€ ë§ºìŒë§ì„ ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.
- "ì²« ë²ˆì§¸ ë…¼ë¬¸", "ì´ë²ˆ ë°°ì¹˜", "ì•ˆë…•í•˜ì„¸ìš”", "ì˜¤ëŠ˜ì€" ê°™ì€ ì§„í–‰ ë©˜íŠ¸ì™€ ìˆœì„œ ë©˜íŠ¸ë¥¼ ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
- ì˜¤ì§ ê° ë…¼ë¬¸ ì„¤ëª… ë³¸ë¬¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

ë¶„ëŸ‰:
- ë…¼ë¬¸ 1í¸ë‹¹ ì•½ 1800~2300ì ë‚´ì™¸ë¡œ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”.

ì–¸ì–´ ê·œì¹™:
- ì‰¼í‘œ(,)ë¡œ í˜¸í¡, ë§ˆì¹¨í‘œ(.)ë¡œ ê°•ì¡°.
- CNN, ViT, GAN, SOTA ë“± ì•½ì–´ëŠ” ì˜ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ê²ƒ.
- diffusion, transformer, attention, encoder, decoder, latent ë“± ì „ë¬¸ ìš©ì–´ëŠ” ë²ˆì—­í•˜ì§€ ë§ê³  ì˜ì–´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ê²ƒ.
- ë™ë£Œ ì—°êµ¬ìì—ê²Œ ì„¤ëª…í•˜ë“¯ ì°¨ë¶„í•œ êµ¬ì–´ì²´.
- ì˜¤ë””ì˜¤ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ êµ¬ì¡°/ìˆœì„œë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë§í•˜ì§€ ë§ ê²ƒ(ì˜ˆ: "A.", "B.", "ì²«ì§¸", "ë‹¤ìŒìœ¼ë¡œ", "ì´ì–´ì„œ").
- ì „ì²´ ë¸Œë¦¬í•‘ì€ ë°˜ë“œì‹œ ê³µì ì¸ ë¼ë””ì˜¤ í†¤ì˜ ì¡´ëŒ“ë§ë¡œ ì‘ì„±í•  ê²ƒ.
- ë°˜ë§, êµ¬ì–´ì²´ ì¶•ì•½, ì¹œê·¼í•œ ëŒ€í™”ì²´(ì˜ˆ: ~í•´ìš”, ~í–ˆì£ )ëŠ” ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ.

ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ì¤€ìˆ˜):
TITLE: <ì˜ë¬¸ ì œëª©>
BODY:
<ë³¸ë¬¸>

(ë…¼ë¬¸ê³¼ ë…¼ë¬¸ ì‚¬ì´ëŠ” ë¹ˆ ì¤„ 2ì¤„)
""".strip()


def call_gpt_text(system_text, user_text, max_tokens):
    resp = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_text},
            {"role": "user", "content": user_text},
        ],
        max_tokens=max_tokens
    )
    return (resp.choices[0].message.content or "").strip()


def synthesize_tts_to_audio(text, tts_chunk_chars=2000, overlap=0):
    chunks = chunk_text_by_chars(text, chunk_chars=tts_chunk_chars, overlap=overlap)
    combined = AudioSegment.empty()
    for chunk in chunks:
        audio_part_response = client.audio.speech.create(
            model=TTS_MODEL,
            voice=TTS_VOICE,
            input=chunk,
            speed=TTS_SPEED
        )
        part_stream = io.BytesIO(audio_part_response.content)
        segment = AudioSegment.from_file(part_stream, format="mp3")
        combined += segment
    return combined


def sanitize_title_for_tts(title):
    if not title:
        return ""
    return re.sub(r"[:\-+/]", ",", title)


def parse_title_body_blocks(text):
    text = (text or "").strip()
    if not text:
        return []

    pattern = r"TITLE:\s*(.*?)\s*BODY:\s*(.*?)(?=(?:\n\s*TITLE:)|\Z)"
    matches = re.findall(pattern, text, flags=re.DOTALL | re.IGNORECASE)

    blocks = []
    for title, body in matches:
        t = title.strip()
        b = body.strip()
        if t and b:
            blocks.append((t, b))

    if blocks:
        return blocks

    fallback = []
    chunks = re.split(r"\n\s*TITLE:\s*", "\n" + text)
    for c in chunks:
        c = c.strip()
        if not c:
            continue
        if "BODY:" in c:
            t, b = c.split("BODY:", 1)
            t = t.strip()
            b = b.strip()
            if t and b:
                fallback.append((t, b))
    return fallback


def assemble_radio_script(full_batches_text, total_papers):
    intro = f"ì•ˆë…•í•˜ì„¸ìš”, ì•„ì´ì•Œì”¨ë¸Œì´ ë©ì‹¤ì˜ ìˆ˜ì„ ì—°êµ¬ ë¹„ì„œì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ì„ ë³„ëœ ì»´í“¨í„° ë¹„ì „ ì‹ ê·œ ë…¼ë¬¸ì€ ì´ {total_papers}ê±´ì…ë‹ˆë‹¤."
    outro = "ì´ìƒìœ¼ë¡œ ì˜¤ëŠ˜ì˜ ë¸Œë¦¬í•‘ì„ ë§ˆì¹˜ê² ìŠµë‹ˆë‹¤."

    all_blocks = []
    for batch_text in full_batches_text:
        all_blocks.extend(parse_title_body_blocks(batch_text))

    script_parts = [intro, ""]
    for i, (title, body) in enumerate(all_blocks, start=1):
        title_tts = sanitize_title_for_tts(title)

        if i == 1:
            transition = "ì²« ë²ˆì§¸ë¡œ ì‚´í´ë³¼ ë…¼ë¬¸ì…ë‹ˆë‹¤."
        else:
            transition = f"{i}ë²ˆì§¸ ë…¼ë¬¸ì…ë‹ˆë‹¤."

        script_parts.append(transition)
        script_parts.append(f"ë…¼ë¬¸ ì œëª©ì€ {title_tts} ì…ë‹ˆë‹¤.")
        script_parts.append(body)
        script_parts.append("")

    if len(all_blocks) < total_papers:
        script_parts.append("ì¼ë¶€ ì›ê³  ìƒì„±ì´ ëˆ„ë½ë˜ì–´, ìƒì„±ëœ ë¶€ë¶„ê¹Œì§€ë§Œ ì œê³µë©ë‹ˆë‹¤.")
        script_parts.append("")

    script_parts.append(outro)
    return "\n".join(script_parts).strip()


def compute_author_score_0_100(s2_paper):
    # Author score ë¹„ì¤‘ì„ ì˜¬ë¦¬ê¸° ìœ„í•´, 0~100ìœ¼ë¡œ ê°•í•˜ê²Œ ìŠ¤ì¼€ì¼
    authors = (s2_paper.get("authors") or [])
    if not authors:
        return 5

    def one_author(a):
        h = a.get("hIndex") or 0
        pc = a.get("paperCount") or 0
        cc = a.get("citationCount") or 0

        score = 0

        # h-index
        if h >= 60:
            score += 55
        elif h >= 40:
            score += 45
        elif h >= 25:
            score += 35
        elif h >= 15:
            score += 25
        elif h >= 8:
            score += 15
        else:
            score += 8

        # ìƒì‚°ì„±
        if pc >= 150:
            score += 20
        elif pc >= 80:
            score += 15
        elif pc >= 40:
            score += 10
        elif pc >= 15:
            score += 6
        else:
            score += 3

        # ëˆ„ì  ì¸ìš©(ê±°ì¹ ê²Œ)
        if cc >= 20000:
            score += 25
        elif cc >= 8000:
            score += 18
        elif cc >= 2000:
            score += 12
        elif cc >= 300:
            score += 7
        else:
            score += 3

        return min(100, score)

    scores = sorted([one_author(a) for a in authors if a], reverse=True)
    if not scores:
        return 5

    if len(scores) == 1:
        base = scores[0]
    else:
        base = (scores[0] * 0.7) + (scores[1] * 0.3)

    return int(round(max(0, min(100, base))))


def compute_signal_score_0_100(arxiv_paper):
    # ë¬´ê±°ìš´ ê¸°ì¤€ ì œê±°: ì´ˆë¡ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ê°€ë²¼ìš´ ì‹ í˜¸ë§Œ ì‚¬ìš©
    abs_ = (arxiv_paper.summary or "").lower()

    good = [
        "benchmark", "dataset", "ablation", "analysis", "failure", "limitation",
        "code", "github", "open-source", "open source", "reproduc",
        "vision-language", "multimodal", "distillation", "foundation model",
        "detection", "segmentation", "tracking", "3d", "depth", "pose", "video"
    ]

    hits = 0
    for k in good:
        if k in abs_:
            hits += 1

    # 0~100ìœ¼ë¡œëŠ” ê³¼í•˜ë‹ˆ ì™„ë§Œí•˜ê²Œ
    if hits >= 10:
        score = 80
    elif hits >= 7:
        score = 65
    elif hits >= 4:
        score = 50
    elif hits >= 2:
        score = 35
    elif hits >= 1:
        score = 25
    else:
        score = 15

    return score


def compute_penalty_0_30(arxiv_paper, s2_paper):
    # ê³¼ì¥/ê·¼ê±°ë¶€ì¡±ë§Œ ê°€ë³ê²Œ ê°ì 
    abs_ = (arxiv_paper.summary or "").lower()
    penalty = 0

    hype = ["revolutionary", "breakthrough", "novel paradigm", "unprecedented", "game-changing"]
    if any(k in abs_ for k in hype):
        penalty += 8

    if ("experiment" not in abs_) and ("evaluation" not in abs_) and ("benchmark" not in abs_) and ("dataset" not in abs_):
        penalty += 10

    authors = (s2_paper.get("authors") or [])
    if authors:
        hs = [(a.get("hIndex") or 0) for a in authors if a]
        if hs and max(hs) < 5:
            penalty += 12

    return max(0, min(30, penalty))


def total_score_0_100(arxiv_paper, s2_paper):
    # Author score ë¹„ì¤‘ ìƒìŠ¹ (80%), signal (20%), penalty ì°¨ê°
    a = compute_author_score_0_100(s2_paper)
    s = compute_signal_score_0_100(arxiv_paper)
    p = compute_penalty_0_30(arxiv_paper, s2_paper)

    score = (a * 0.80) + (s * 0.20) - p
    return int(round(max(0, min(100, score))))


def select_top_k_papers(valid_papers, k):
    # arXiv id ìˆ˜ì§‘
    arxiv_ids = []
    paper_by_id = {}

    for p in valid_papers:
        aid = arxiv_id_from_entry_id(getattr(p, "entry_id", "") or "")
        if not aid:
            aid = arxiv_id_from_entry_id(getattr(p, "pdf_url", "") or "")
        if aid:
            arxiv_ids.append(aid)
            paper_by_id[aid] = p

    # S2 batch í˜¸ì¶œ
    s2_map = {}
    for chunk in chunk_list(arxiv_ids, 200):
        s2_map.update(fetch_s2_papers_batch(chunk))

    scored = []
    for aid in arxiv_ids:
        p = paper_by_id.get(aid)
        s2 = s2_map.get(aid)

        if not p:
            continue

        if not s2:
            # S2 ëˆ„ë½ ì‹œ ë‚®ì€ ì ìˆ˜
            scored.append((10, aid))
            continue

        sc = total_score_0_100(p, s2)
        scored.append((sc, aid))

    scored.sort(reverse=True, key=lambda x: x[0])
    top = scored[:k]
    top_ids = [aid for _, aid in top]

    selected = [paper_by_id[aid] for aid in top_ids if aid in paper_by_id]
    return selected, scored


def run_bot():
    base_path = os.path.dirname(os.path.abspath(__file__))
    audio_dir = os.path.join(base_path, "audio")
    os.makedirs(audio_dir, exist_ok=True)

    seoul_tz = timezone("Asia/Seoul")
    et_tz = timezone("America/New_York")

    now_kst = datetime.datetime.now(seoul_tz)
    now_et = now_kst.astimezone(et_tz)

    window_start_et, window_end_et = get_last_announce_window_et(now_et)

    search = arxiv.Search(
        query="cat:cs.CV",
        max_results=ARXIV_MAX_RESULTS,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )

    candidates = []
    for p in search.results():
        p_et = p.published.astimezone(et_tz)
        if window_start_et <= p_et < window_end_et:
            candidates.append(p)

    if not candidates:
        print("í•´ë‹¹ announce windowì—ì„œ ìƒˆë¡œ ì˜¬ë¼ì˜¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    top_papers, scored_all = select_top_k_papers(candidates, TOP_K)

    if not top_papers:
        print("Top ë…¼ë¬¸ì„ ì„ íƒí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤(S2 ëˆ„ë½ ë“±).")
        return

    system_summary = "ë‹¹ì‹ ì€ ì—°êµ¬ì‹¤ì˜ ìˆ˜ì„ ì—°êµ¬ ë¹„ì„œì´ì AI ì „ë¬¸ ë¼ë””ì˜¤ ì§„í–‰ìì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ìš”ì•½ê³¼ 3ë¶„ ëŒ€ë³¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì¡´ëŒ“ë§ì„ ìœ ì§€í•´ ì£¼ì„¸ìš”."
    system_full = "ë‹¹ì‹ ì€ ì—°êµ¬ì‹¤ì˜ ìˆ˜ì„ ì—°êµ¬ ë¹„ì„œì´ì AI ì „ë¬¸ ë¼ë””ì˜¤ ì§„í–‰ìì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ë…¼ë¬¸ ë³¸ë¬¸ ìŠ¤í¬ë¦½íŠ¸ë§Œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì¡´ëŒ“ë§ì„ ìœ ì§€í•´ ì£¼ì„¸ìš”."

    user_summary = prompt_summary_and_3min(top_papers)
    summary_out = call_gpt_text(system_summary, user_summary, MAX_OUT_TOKENS_SUMMARY)

    if "[3ë¶„ëŒ€ë³¸]" in summary_out:
        summary_text = summary_out.split("[3ë¶„ëŒ€ë³¸]")[0].replace("[ìš”ì•½]", "").strip()
        audio_script_3min = summary_out.split("[3ë¶„ëŒ€ë³¸]")[1].strip()
    else:
        summary_text = summary_out.replace("[ìš”ì•½]", "").strip()
        audio_script_3min = ""

    paper_batches = [top_papers[i:i + BATCH_SIZE_FULL] for i in range(0, len(top_papers), BATCH_SIZE_FULL)]
    total_batches = len(paper_batches)

    full_batches_text = []
    for idx, batch in enumerate(paper_batches, start=1):
        start_index = (idx - 1) * BATCH_SIZE_FULL + 1
        user_full = prompt_full_body_for_batch(batch, idx, total_batches, start_index)
        batch_text = call_gpt_text(system_full, user_full, MAX_OUT_TOKENS_FULL_PER_BATCH)
        full_batches_text.append(batch_text)

    audio_script_full = assemble_radio_script(full_batches_text, total_papers=len(top_papers))

    combined_audio = synthesize_tts_to_audio(
        audio_script_full,
        tts_chunk_chars=TTS_CHUNK_CHARS,
        overlap=TTS_CHUNK_OVERLAP
    )

    today_date = now_kst.strftime("%Y%m%d")
    file_name_full = f"CV_Daily_Briefing_{today_date}.mp3"
    full_file_path = os.path.join(audio_dir, file_name_full)
    combined_audio.export(full_file_path, format="mp3")

    file_name_3min = f"3Min_Summary_{today_date}.mp3"
    full_file_path_3min = os.path.join(audio_dir, file_name_3min)

    if audio_script_3min.strip():
        audio_3min = synthesize_tts_to_audio(
            audio_script_3min,
            tts_chunk_chars=TTS_CHUNK_CHARS,
            overlap=TTS_CHUNK_OVERLAP
        )
        audio_3min.export(full_file_path_3min, format="mp3")
    else:
        open(full_file_path_3min, "wb").close()

    audio_url = f"https://raw.githubusercontent.com/{GITHUB_USER}/{GITHUB_REPO}/main/audio/{file_name_full}"
    audio_url_3min = f"https://raw.githubusercontent.com/{GITHUB_USER}/{GITHUB_REPO}/main/audio/{file_name_3min}"

    page_title = f"{now_kst.strftime('%Y-%m-%d')} ëª¨ë‹ ë¸Œë¦¬í•‘ (Top {len(top_papers)})"

    notion_children = [
        {"object": "block", "type": "callout",
         "callout": {"rich_text": [{"type": "text", "text": {"content": f"arXiv announce window ê¸°ì¤€ í›„ë³´ {len(candidates)}ê°œ ì¤‘, Author ì¤‘ì‹¬ ì ìˆ˜ë¡œ ìƒìœ„ {len(top_papers)}ê°œë§Œ ë¸Œë¦¬í•‘í•©ë‹ˆë‹¤."}}],
                    "icon": {"emoji": "ğŸ§­"}}},
        {"object": "block", "type": "heading_2",
         "heading_2": {"rich_text": [{"type": "text", "text": {"content": "ë…¼ë¬¸ í•µì‹¬ ìš”ì•½"}}]}}
    ]

    for part in split_notion_text(summary_text, max_len=1900):
        notion_children.append({
            "object": "block",
            "type": "paragraph",
            "paragraph": {"rich_text": [{"type": "text", "text": {"content": part}}]}
        })

    notion_children += [
        {"object": "block", "type": "divider", "divider": {}},
        {"object": "block", "type": "heading_2",
         "heading_2": {"rich_text": [{"type": "text", "text": {"content": "ë…¼ë¬¸ ì›ë¬¸ ë§í¬ (Top 10)"}}]}}
    ]

    for i, p in enumerate(top_papers):
        notion_children.append({
            "object": "block", "type": "bulleted_list_item",
            "bulleted_list_item": {
                "rich_text": [
                    {"type": "text", "text": {"content": f"{i + 1}. {p.title} "}},
                    {"type": "text", "text": {"content": "PDF", "link": {"url": p.pdf_url}},
                     "annotations": {"bold": True, "color": "blue"}}
                ]
            }
        })

    notion.pages.create(
        parent={"database_id": DATABASE_ID},
        properties={
            "ìš”ì•½ & ë…¼ë¬¸ë§í¬": {"title": [{"text": {"content": page_title}}]},
            "ë‚ ì§œ": {"date": {"start": now_kst.date().isoformat()}},
            "ì „ì²´ ë¸Œë¦¬í•‘": {
                "rich_text": [
                    {
                        "type": "text",
                        "text": {
                            "content": "â–¶ ì „ì²´ ë¸Œë¦¬í•‘ ë‹¤ìš´",
                            "link": {"url": audio_url}
                        }
                    }
                ]
            },
            "3ë¶„ ìš”ì•½": {
                "rich_text": [
                    {
                        "type": "text",
                        "text": {
                            "content": "â–¶ 3ë¶„ ìš”ì•½ ë‹¤ìš´",
                            "link": {"url": audio_url_3min}
                        }
                    }
                ]
            }
        },
        children=notion_children
    )

    print(f"ì™„ë£Œ: í›„ë³´ {len(candidates)}ê°œ ì¤‘ Top {len(top_papers)}ê°œ ìƒì„±")


if __name__ == "__main__":
    run_bot()
